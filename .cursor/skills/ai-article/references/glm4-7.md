# 实测GLM-4.7，我想给他当股东了

大家好，我是二哥呀。

这几天，相信大家肯定都被一个产品刷屏了。

GLM-4.7。

就是这个后端工程能力媲美 Claude Sonnet 的国产大模型。

![GLM-4.7的截图](https://cdn.paicoding.com/stutymore/glm4-7-4ca8dabe462f354687f5f6028c6eb81b.png)

总参数 355B，专门面向 Coding 场景强化了编码、长程任务规划等能力，目前已在 Hugging Face、ModelScope 开源部署！

![榜一](https://files.mdnice.com/user/3903/8153c477-d26f-4e18-a582-a7c186e32082.jpg)

真不是我在刻意吹捧。你瞧，老外都在 x 上盛赞：GLM-4.7 超越了 Claude-Sonnet-4.5 和 GPT-5。

![老外盛赞 GLM-4.7](https://files.mdnice.com/user/3903/a9f9aaea-1137-4298-b02b-26dc986b30f2.jpg)

这是一个了不起的成绩！

下面是我通过 GLM-4.7 完成的一个 Agent 项目，可以实现工作流的拖拉编排。

![](https://files.mdnice.com/user/3903/76d859ed-c1f5-457b-a7b2-edfaef18fbc6.png)

## 01、使用 Claude Code 接入 GLM-4.7

搞不懂，Claude 明明很封闭，直接断供 Open Code，逼着 Clawdbot 改名 Moltbot。

但 Claude Code 的模型调用层，却是可配置、可替换的。我们只需要在 `.claude/settings.json` 中把 ANTHROPIC_AUTH_TOKEN 替换为 GLM-4.7 的 API Key 就可以了。

另外一个配置项 ANTHROPIC_BASE_URL 的值固定为 `https://open.bigmodel.cn/api/anthropic`。

![](https://files.mdnice.com/user/3903/b2faefbb-9476-4128-8e13-299a9366ec8c.png)

一点都不难吧？

保存后，重启 Claude Code，输入 `/status`，如果看到 BigModel.cn 的身影，就说明配置成功了。

![](https://files.mdnice.com/user/3903/0e2ae8a6-d341-4b3c-9183-00ac273aaf7a.png)

给自己比个耶吧！

从此以后，你将拥有一个无所不能的 Agent，可以帮你写代码、修 bug、读源码，甚至可以帮你搭建项目骨架、部署生产环境上线等。

## 02、GLM-4.7 的后端工程能力

前置环境搞定后，我们直接来新增需求，很简单一句话，我们看看 GLM-4.7 是否能够通情达理，get 到我们的诉求。

> 我现在需要在大模型节点下新增一个智谱节点

tips：拒绝花里胡哨的提示词，真正检验大模型的能力😄

![](https://files.mdnice.com/user/3903/33109199-6cac-4113-b0a7-48c71f6db686.png)


能看到，GLM-4.7 会先探索我们的代码库，了解当前大模型节点的实现结构。

这一点至关重要，就好像我们打算出远门，总要去地图上看看路线，是吧，搞清楚目标和路线后再动手，免得返工。

遇到需要权限的诉求，我们直接给它。

![](https://files.mdnice.com/user/3903/b3240935-eee8-40fa-b90a-5a1132a40289.png)

搞清楚工作流的执行引擎后，GLM-4.7 开始查看数据库和前端代码，非常严谨。

![](https://files.mdnice.com/user/3903/7972d192-c146-42a4-a450-3ab5d52e121c.png)

前后端的代码+数据库搞清楚后，开始正式写代码。真正做到了“先思考、再行动”。

![](https://files.mdnice.com/user/3903/55396f3c-88da-49b9-a7ce-2bf1ca0d3404.png)

有代码需要调整的地方，也会清楚的告诉我们。

![](https://files.mdnice.com/user/3903/ba6c1ade-1c9f-4314-923f-9de0bb765345.png)

我选择全权交给 GLM-4.7 来处理，我只要最后的结果，不过在历史上下文当中，我们也可以再次确认都在哦了哪些修改，一目了然。

![](https://files.mdnice.com/user/3903/4213fa33-9eb4-4083-b5a0-b73a7a5b0854.jpg)

和其他模型有很大的不同，GLM-4.7 非常非常负责任，除了完成你需要他干的活，还会自动检查其他哪些地方需要修改。

![](https://files.mdnice.com/user/3903/b30454f9-1b10-436f-9773-ada8bb2f3a21.png)

所有的任务都搞定后，会给我们列一个任务完成清单。

![](https://files.mdnice.com/user/3903/d47be429-54fe-4ceb-8ea6-6ce750326d38.png)

好，我们直接来看一下效果，完全符合我的预期啊，这可不是闹着玩的 demo 项目，而是一个完全可以运行的工作流编排项目，也是当前最火的 AI 业务之一。

![](https://files.mdnice.com/user/3903/2522c463-b952-476f-9c1f-3dc3c92fa923.png)

竟然一次性搞定了！

从整个交互体验来看，GLM-4.7 更习惯站在“任务交付”的视角，而不是“回答问题”的视角。它给出的不再是零散代码片段，而是：

- 明确的模块划分
- 可运行的代码骨架
- 清楚的依赖关系和执行顺序
- 对边界情况的主动补充说明

这种感觉很像一个工程经验老道的同事，而不是一个只会补全代码的工具。

有意思，有意思，越来越有意思了。

## 03、GLM-4.7 的 bug 修复能力

考验 GLM-4.7 的时候到了，因为真正能拉开差距的地方，真不是第一次写代码的时候，而是出问题之后，能不能把 bug 给收拾干净。

接口报错、鉴权失败、参数不对、依赖冲突，这些东西不可能一次写对。

GLM-4.7 给我的体感非常明确：他不是在“猜怎么改”，而是顺着错误把问题先定位一遍。

比如我在调试过程中遇到了一个很典型的 API 鉴权问题，请求已经发出，但却响应失败了。我没去分析日志，也没有补充额外解释，而是简单粗暴把错误堆栈原样丢给 GLM-4.7。

![](https://files.mdnice.com/user/3903/5c53d34d-112d-4da0-92bd-0893ec5d5a3d.png)

GLM-4.7 没有着急去改代码，而是先判断：

- 是 Key 配置问题，还是 Header 拼错？
- 是鉴权方式不匹配，还是请求路径有误？
- 当前代码和官方示例是否存在实现差异？

这个思考方式，就有点老员工的“工程化”思维了。然后在定位到问题之后，他会明确告诉我们：这里还是那里不对、为什么不对、应该怎么改。

这里顺手分享一个非常实用的小技巧，可能很多人都忽视了。如果你发现第一次请求发出后，还有一些信息需要补充，可以在 GLM-4.7 返回结果前直接再输入一次附加信息。

![](https://files.mdnice.com/user/3903/065c226a-f9c5-4bdb-ad76-a8994b07e577.png)

还有一种场景，不知道大家有没有注意到。在修 bug 时非常常见：当前实现和官方 demo 对不上。

这时候最省事的方式就是——把官方代码示例直接丢给 GLM-4.7。这其实也在考验我们的 Vibe Coding 能力，能不能必要时刻给 AI 一个明确的指示。

省得他在那瞎琢磨。

这一步的价值在于：我们不需要自己一点点 diff 官方和实现的差异，模型会主动帮我们把“意图”和“实现”对齐。

![](https://files.mdnice.com/user/3903/0b841cc0-d755-4056-bade-f10a2834ae5e.png)

当然，bug 修完不是终点，它还会自己检查一遍。这一点，是我觉得 GLM-4.7 非常加分的地方。

当你以为事情已经结束了，它往往会多做一步：扫一遍有没有遗漏的地方。

- 是否还有调用链没改干净？
- 是否有前端或数据库字段需要同步调整？
- 是否存在潜在的边界问题？

![](https://files.mdnice.com/user/3903/532dfeef-3ee2-4404-90d3-6fe8d29b898e.png)

等所有地方都确认无误之后，他才会给你一个完整的修改清单。

讲良心话，我试过其他的模型，这种自我反思的能力就差点意思，经常是你让他改一个地方，他就改一个地方。

殊不知，bug 就藏着这里面，因为有很多时候，代码都是联动的，牵一发而动全身。

老程序员经常告诫我们：勿动屎山，就是因为屎山虽然臭，但它是能运行的。你以为你改好了，实际上 P0 事故可能离你不远了。😄

我们人的精力毕竟有限，但大模型不一样啊，无非就是费点token。

好，我们来看一下修改后的效果，非常 nice。一个完整的输入 →llm 大模型 → 超拟人合成 → 输出的工作流编排就算是完成了。

![](https://files.mdnice.com/user/3903/cd5005f4-eaaf-4198-9ea0-91aa9d3236c7.png)

## 04、GLM-4.7 的前端工程能力

现在的执行状态，是等所有节点都跑完之后，前端一次性展示结果。

但从用户体验上来说，状态流转应该是一个动态过程：

- LLM 节点开始执行 → 执行中 → 执行完成
- 接着切换到 TTS 节点 → 执行中 → 执行完成
- 最后到结束节点，整体流程结束

我们把需求直接扔给 GLM-4.7：

> 现在有个问题，我发现，执行状态、节点执行结果都是等所有节点都执行完后才显示出来的，实际上这应该是一个动态的过程，llm 节点开始执行的时候执行状态就切到 llm 节点开始执行、执行中、执行结束，然后到 tts 的开始执行、执行中、执行结束，最后到结束节点，这应该是实时响应的。前后端需要联调起来。

面对这个问题，GLM-4.7 并没有一上来就写代码，而是先明确了一点：这是一个典型的「执行状态实时推送」问题，技术上可以用 SSE，也可以用 WebSocket。

![](https://files.mdnice.com/user/3903/b5479227-d9fe-446e-96db-171d552c15d6.png)

为了方便后期能够主动中断工作流的执行，GLM-4.7 选择了 WebSocket。

![](https://files.mdnice.com/user/3903/97494a97-0e94-40ef-9ce6-7beb2479dd94.png)

GLM-4.7 并没有只盯着前端，而是从后端执行引擎开始改起：

- 后端增加 WebSocket 依赖和基础配置
- 新增 WebSocket Handler，用于推送节点执行状态
- 在执行引擎中，把“节点生命周期”拆成可感知的事件
- 在节点开始、执行中、结束等关键节点，主动向前端推送状态

后端改完之后，它才开始动前端。前端这边的改动也非常清晰：

- 建立 WebSocket 连接
- 监听后端推送的执行事件
- 根据节点 ID 实时更新节点状态
- 让画布上的节点“活”起来，而不是等结果一次性刷新

![](https://files.mdnice.com/user/3903/b68ce1a3-7b00-40b8-aa0e-0d6e92f0d78a.jpg)

有一个细节，我觉得特别加分。在节点执行完成后，GLM-4.7 顺手把节点执行结果里的 JSON 文本做了格式化展示，而不是直接把一坨字符串甩给用户。

![](https://files.mdnice.com/user/3903/e494e38a-536e-4308-bded-a88ca17ef1bb.png)

这个动作看起来很小，但非常贴心。

我录了一个完整的视频，大家可以看看，非常完美。从输入 → LLM 执行 → TTS 合成 → 输出完成，
每一步都是实时可见的。

【视频】

到这一刻，我心里已经笃定了：GLM-4.7 已经不只是“会写前端代码”，还能理解前端在整个系统中的位置和责任。

除了硬核的实战代码能力之外，这次的 GLM-4.7 还进一步提升了前端审美。

我们直接来让它来帮我们把用户界面改造为赛博朋克风。

![](https://files.mdnice.com/user/3903/8ed438ce-2ad4-4980-9768-b3ffb425ecda.png)

当然了，这个过程中也出现了一些其他的小问题，但好在经过我和 GLM-4.7 的通力合作，算是都解决了。来看一下最后呈现的效果吧。

![](https://files.mdnice.com/user/3903/eb753b08-4459-4190-916d-8fbdc2eeb0f8.jpg)

是不是很酷？

## 05、Coding Plan 年包计划

如果只让我用一句话来总结真实体感，那就是：

**GLM-4.7，已经坐实了 Claude 的最佳平替，绝不是嘴上说说那种。** 

从 GLM-4.5 出来能承接一部分编程小项目，再到 GLM-4.6 更强的编程，智谱今年在 Coding 上的发力是完全没有想到的力度。

不是在榜单里跑分，也不是写几个 demo case，而是放进真实工程、真实 Agent、真实联调环境里之后，你会明显感觉到一件事——它确实是一个称心&顺手的「工程生产力工具」了。

- 它能读懂真实项目结构，而不是只看单文件；
- 它习惯从“任务交付”而不是“回答问题”的角度出发；
- 它会主动检查遗漏、修 bug、补联调，而不是写完就走；
- 它在前后端协同、状态流转、工程约束这些地方，明显是有经验积累的。

我自己是订阅了智谱的 Coding Plan 年包计划，平常开发基本上不用担心 token 的用量问题，性价比这一点，对比一下 Claude 那是香的没得说。

![](https://files.mdnice.com/user/3903/b2584a71-d8b5-4c66-8b66-e4e74ac1e9e3.png)

- 更稳定的 Coding 专用模型调度策略
- 对长上下文、复杂任务拆解的明显优化
- 在 Claude Code、IDE 类工具里的兼容性持续增强
- 对 Agent 场景、工具调用、任务连续性的支持越来越完整

从一个开发者的角度说，这种投入是非常值得的。

如果你现在正好在做 Agent、做工作流、做复杂业务系统，或者你只是单纯想提升自己的工程效率、在 AI 时代把生产力再往上抬一档。

那 GLM-4.7 **绝对是一个明智的选择。**

## 06、ending

我坚信，AI 编程，一定会有光明的未来。

哪怕现阶段还有很多的不完美。

但日子还长，只要抱有希望，抱有对这个世界的热爱。

我们就一定能用 AI Coding 实现更多的创业，挖掘更多的商业价值。

给这个世界创造更多的可能性。

如果这篇内容对你有用，记得点赞，转发给需要的人。

我们下期见！


